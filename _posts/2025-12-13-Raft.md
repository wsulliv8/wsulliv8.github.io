---
layout: post
title: "From Chaos to Consensus: Building Raft from Scratch in Go"
image:
excerpt: Distributed systems are hard. How do we handle timing difference, network partitions, and node failures? I will answer all of these questions and more in this thorough deep-dive into distributed systems theory and practical applications.
---

Building a distributed system is fundamentally different from building a single-server application. When you have one server, you can rely on a single source of truth, predictable execution order, and straightforward error handling. This simplicity is paid for in full by reduced fault tolerance, availability, safety, and scalability. A distributed system is able to provide a solution for all of these problems. Sounds great, right? Well, not always. Make a system distributed comes with a whole host of new potential problems such as split-brain scenarios, failed (or byzantine) nodes, and network failures. These problems are complex enough to warrant an entire field of study in computer science. 

In this blog, we will shed light on the Raft consensus algorithm by visiting core concepts in distributed systems theory, comparing Raft to other solutions such as Paxos, and building a distributed key-value store with Raft consensus from the ground up. 

## The problem

Imagine you're building a database that needs to handle millions of requests per second. A single server can't handle that load, so you replicate your data across multiple servers. But now you have a new problem: how do you ensure all servers agree on what the data should be?

This is the consensus problem, and it's one of the most fundamental challenges in distributed systems. The Two Generals Problem illustrates why this is so difficult: two generals need to coordinate an attack, but their messengers can be intercepted. Even with perfect communication, if messages can be lost, there's no algorithm that can guarantee both generals will agree on the attack time. [Dr. Martin Kleppman](https://martin.kleppmann.com/) (Associate Professor at the University of Cambridge and author of [Designing Data Intensive Applications](https://unidel.edu.ng/focelibrary/books/Designing%20Data-Intensive%20Applications%20The%20Big%20Ideas%20Behind%20Reliable,%20Scalable,%20and%20Maintainable%20Systems%20by%20Martin%20Kleppmann%20(z-lib.org).pdf)) provides an intuitive diagram of the Two Generals Problem:

<div class="image-container"><img src="/assets/images/raft/two-generals.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-1: Two generals must decide on the same course of action </em></p>
Fig-1 shows why it is impossible for the generals to coordinate an attack if messages can be lost. If general 1 does not hear back from general 2 then it is either because general 1's message was captured or general 2's message was captured, but its impossible to determine which is the case.

In our case, we're not coordinating generals, we're coordinating servers. But the core challenge remains: how do multiple servers agree on the order of operations when messages can be lost, servers can crash, and the network can partition?

## The system model

Before diving into Raft, we need to understand what assumptions we can make about the system; these assumptions combine to form a "system model". Our assumed system model fundamentally shapes what algorithms are possible.

### 1) Network

**Reliable Network**: Messages are never lost, duplicated, or reordered. This is the ideal case, but unrealistic in practice.

**Fair-Loss Network**: Messages can be lost, but if you keep retrying, eventually the message will be delivered. This is what Raft assumes, and it's a reasonable model for most real-world networks (TCP provides this guarantee).

**Arbitrary Network**: Messages can be corrupted, duplicated, or delivered out of order. This is the Byzantine failure model, which requires more complex algorithms like [PBFT (Practical Byzantine Fault Tolerance)](http://pmg.csail.mit.edu/papers/osdi99.pdf). [Bitcoin](https://www.ussc.gov/sites/default/files/pdf/training/annual-national-training-seminar/2018/Emerging_Tech_Bitcoin_Crypto.pdf) is a popular Byzantine fault-tolerant network.

>**At-Most-Once vs. At-Least-Once Semantics:** In a local function call, code executes exactly once. In a distributed system, an RPC (Remote Procedure Call) is merely a "suggestion" to the network. When a Client sends a `Put` request to the Leader and times out, three things could have happened:

>1. The request never reached the Leader.
    
>2. The Leader processed it, but the response was lost.
    
>3. The Leader processed it, crashed, and the new Leader doesn't know.
    

>If the client simply retries (At-Least-Once semantics), applying the same `Put(x, 5)` twice is safe (idempotent), but `Append(x, 5)` is not. Idempotency can also be implemented via keys (client IDs and sequence numbers) to ensure Exactly-Once semantics for a state machine.

### 2) Node Failure

**Crash-Stop**: Nodes fail by stopping completely and never recovering. This is the simplest failure model.

**Crash-Recovery**: Nodes can crash and then recover, potentially with corrupted state. Our implementation handles this through persistent state that survives crashes.

**Byzantine Failures**: Nodes can behave arbitrarily (i.e., they might lie, send conflicting messages, or collude). [The Byzantine Generals Problem](https://lamport.azurewebsites.net/pubs/byz.pdf)by Leslie Lamport shows that with `f` Byzantine nodes, you need at least `3f+1` total nodes to reach consensus vice a typical system's `2f+1` requirement. Bitcoin's proof-of-work is one approach to Byzantine consensus, while PBFT provides a more traditional solution. I've included a depiction of the Byzantine General's problem from Leslie Lamport's paper below.


<div class="image-container"><img src="/assets/images/raft/byzantine.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-2: The commander node is byzantine (i.e., it attempts to trick its lieutenant nodes in agreeing to differing courses of action) </em></p>

### 3) Timing 

**Synchronous**: There's a known upper bound on message delivery time. This makes consensus easier, but real networks aren't synchronous.

**Partially Synchronous**: The system is asynchronous most of the time, but there are periods of synchrony. Eventually, messages will be delivered within a reasonable time.

**Asynchronous**: No timing guarantees whatsoever. The [FLP Impossibility Theorem (Fischer, Lynch, Paterson)](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf) proves that consensus is impossible in a fully asynchronous system with even one crash failure. 

>**Raft's Assumptions**: Fair-loss network, crash-recovery nodes, partially synchronous timing. These assumptions allow Raft to provide strong consistency guarantees while remaining practical to implement.

### Consistency Models

Our Raft implementation provides linearizability (also known as strong/strict consistency or atomic consistency) which is the strongest form of consistency. A linearizable system guarantees that operations appear to execute atomically at some point between their invocation and response. This means all nodes see operations in the same order.

> Note that Raft provides linearizable writes by default since write are only sent to the leader and the leader doesn't respond until after the entry is committed. Reads are not linearizable unless the application quorum reads is implemented. A Raft system without linearizable reads would be considered "sequentially consistent" in that all clients see the same global order of updates, but the order might not match real time.

Serializability is weaker; it only guarantees that the execution is equivalent to some serial execution, but doesn't specify which one. Most SQL databases provide serializability, but not linearizability.

Other types of consistency models include causal consistency which captures the "happens-before" relationship between nodes, and eventual consistency which guarantees that eventually all reads will return the last updated value (assuming no new updates are made) but reads in the interim can return anything. 

We will discuss a few famous implementations of eventually consistent systems later.

## Trade-offs: The CAP Theorem

[The CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem, proposed by Eric Brewer, states that in a distributed system, you can guarantee either:

- **Consistency**: All nodes see the same data at the same time

_OR_

- **Availability**: The system remains operational even if nodes fail

_in the presence of a_

- **Partition**: A disconnect in the network

A network partition occurs when the network splits into multiple disconnected groups. For example, if you have 5 servers and the network splits them into groups of 3 and 2, you have a partition.


<div class="image-container"><img src="/assets/images/raft/network-partition.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-3: A 3:2 network partition. </em></p>

>Many developers misunderstand CAP as 'Pick 2 of 3.' In reality, **Partition Tolerance (P)** is non-negotiable in distributed systems because networks are unreliable. The theorem really asks: **'When the network fails, do we stop processing (CP) or do we accept conflicting writes (AP)?**


<div class="image-container"><img src="/assets/images/raft/cap.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-4: A network partition is non-negotiable in a distributed system. </em></p>

### Availability vs. Consistency

**High Availability Systems**: Amazon's [Dynamo](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf), [Bayou](https://people.eecs.berkeley.edu/~brewer/cs262b/update-conflicts.pdf) (a predecessor to modern collaboration software), and systems using [CRDTs (Conflict-free Replicated Data Types)](https://crdt.tech/papers.html) prioritize availability. Google Docs uses CRDTs to allow collaborative editing even when users are offline. These systems accept that different users might temporarily see different states.

**High Consistency Systems**: Our Raft implementation, along with [etcd](https://etcd.io/) and [ZooKeeper](https://classpages.cselabs.umn.edu/Fall-2020/csci8211/Papers/Distributed%20Systems%20Zab-%20High-performance%20broadcast%20for%20primary-backup%20systems.pdf), prioritizes consistency. If a network partition occurs, the minority partition becomes unavailable to prevent split-brain scenarios. This is the right choice when data correctness is more important than availability.

## Consensus

Consensus is the process by which multiple nodes agree on a value or sequence of values. It's the foundation that enables State Machine Replication (SMR), where we replicate a state machine across multiple nodes and ensure they all execute the same sequence of operations.

In SMR, we maintain a replicated log of operations. Each node applies these operations to its local state machine in the same order, ensuring all nodes end up in the same state. This is exactly what Raft does; it maintains a replicated log that all nodes agree on.

### Why consensus matters

Consensus solves three critical problems:

1. **Leader Failure**: When the leader crashes, consensus ensures a new leader is elected and the system continues operating.

2. **Network Partitions**: Consensus prevents split-brain scenarios where multiple nodes think they're the leader.

3. **Split-Brain Prevention**: By requiring a majority, consensus ensures at most one partition can make progress.

### Types of Consensus Algorithms

**[Paxos](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)**: The original consensus algorithm, proposed by Leslie Lamport. It's correct but notoriously difficult to understand and implement. Many systems claim to use Paxos but actually use simplified variants.

**[Raft](https://raft.github.io/raft.pdf)**: Designed to be as correct as Paxos but easier to understand. It separates leader election, log replication, and safety into distinct components. 

**Zab (Zookeeper Atomic Commit)**: Used by ZooKeeper. Similar to Raft but designed specifically for ZooKeeper's use case.

**PBFT (Practical Byzantine Fault Tolerance)**: Handles Byzantine failures but requires `3f+1` nodes to tolerate `f` failures.

**Why Raft over Paxos**: Raft's design philosophy emphasizes understandability. The algorithm is broken into clear components (leader election, log replication, safety), making it easier to reason about, implement, and debug. As the Raft paper states: "We set out to improve upon Paxos by providing a general-purpose consensus algorithm that is more understandable."

### Stateful vs. Stateless Design

Our Raft KV store is stateful such that each server maintains persistent state (the log, current term, voted-for). This is similar to [Google File System (GFS)](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf) chunkservers, which maintain state about which chunks they store.

Stateful systems are sensitive to ordering (i.e., operations must be applied in the correct order). This is why Raft's log ordering is so critical.

Stateless systems such as early implementations of the [Network Filesystem (NFS)](https://web.stanford.edu/class/cs240/readings/nfs.pdf) don't maintain server-side state. While simpler, they can't provide strong consistency guarantees. Modern systems are increasingly stateful because state enables better performance and stronger guarantees.

## Raft Algorithm and Practical Implementation

For an intuitive and interactive visualization of the Raft protocol, check out [this site](https://raft.github.io/). Check out our implementation on [GitHub](https://github.com/wsulliv8/raft-kv-store-public/tree/main).

### Why We Chose Go

[Go's](https://go.dev/) concurrency primitives make it ideal for implementing distributed systems:

**Goroutines**: Lightweight threads that make it easy to handle concurrent RPCs, background tasks (like heartbeats), and log application.

**Channels**: Provide safe communication between goroutines. We use channels to notify when log entries are committed and ready to be applied.

**Mutexes**: `sync.Mutex` provides fine-grained locking for protecting shared state. We use separate locks for Raft state and RPC clients to prevent deadlocks.

**WaitGroups**: Coordinate goroutines, useful when collecting votes from multiple servers during leader election.

Here's how we use goroutines for background heartbeats:

```go
func (s *Server) becomeLeaderLocked() {
    s.nodeRole = roleLeader
    s.leaderId = int32(s.id)

    // Start heartbeat ticker
    s.heartbeatTicker = time.NewTicker(100 * time.Millisecond)
    go func() {
        for range s.heartbeatTicker.C {
            s.sendHeartbeats()
        }
    }()

    // Append no-op entry to establish leadership
    noopEntry := &pb.LogEnt{
        Term: s.currTerm,
        Key:  "",
        Value: "",
    }
    s.log = append(s.log, noopEntry)
    s.persistState()
}
```

### Types of Raft Nodes

Raft nodes operate in one of three roles:

1. **Follower**: The default state. Followers receive log entries from the leader and vote in elections.

2. **Candidate**: A node that's trying to become leader. It requests votes from other nodes.

3. **Leader**: Handles all client requests, replicates log entries to followers, and commits entries once they're replicated to a majority.

Nodes transition between these roles based on timeouts and RPCs:

```go
type role string

const (
    roleFollower  role = "follower"
    roleCandidate role = "candidate"
    roleLeader    role = "leader"
)
```

The Raft paper provides a useful state transition diagram for each of the three states:


<div class="image-container"><img src="/assets/images/raft/raft-nodes.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-5: Raft node state transitions and triggering events.</em></p>


### Log Replication

Log replication is how Raft ensures all nodes agree on the sequence of operations. The leader appends entries to its log, then replicates them to followers. Once a majority of followers have replicated an entry, it's committed and can be applied to the state machine.

**Quorum Requirement**: With `2n+1` nodes, we need `n+1` nodes (a majority) to agree. This ensures that any two majorities overlap in at least one node, preventing split-brain scenarios.

Here's our log replication implementation:

```go
func (s *Server) AppEnt(_ context.Context, req *pb.AppEntArg) (*pb.AppEntRep, error) {
    s.mu.Lock()
    defer s.mu.Unlock()

    // Reject if term is stale
    if req.Term < s.currentTerm {
        return &pb.AppEntRep{Term: s.currentTerm, Success: false}, nil
    }

    // Update term if needed
    if req.Term > s.currentTerm {
        s.becomeFollowerLocked(req.Term)
    }

    // Check log consistency
    if req.PrevLogIndex > 0 {
        if int(req.PrevLogIndex) >= len(s.log) ||
           s.log[req.PrevLogIndex].Term != req.PrevLogTerm {
            return &pb.AppEntRep{Term: s.currentTerm, Success: false}, nil
        }
    }

    // Append new entries (with conflict resolution)
    for i, entry := range req.Entries {
        logIndex := int(req.PrevLogIndex) + i + 1
        if logIndex < len(s.log) && s.log[logIndex].Term != entry.Term {
            // Truncate conflicting entries
            s.log = s.log[:logIndex]
            s.log = append(s.log, entry)
        } else if logIndex >= len(s.log) {
            s.log = append(s.log, entry)
        }
    }

    // Update commit index
    if req.LeaderCommit > s.commitIndex {
        s.commitIndex = min(req.LeaderCommit, int32(len(s.log)-1))
    }

    s.persistState()
    return &pb.AppEntRep{Term: s.currentTerm, Success: true}, nil
}
```

The key insight is the log matching property: if two logs contain an entry with the same index and term, they are identical up to that point. This allows Raft to detect and resolve conflicts by truncating divergent log entries.

### Leader Election

Leader election happens when a follower doesn't hear from the leader for a while. It becomes a candidate and requests votes. If it receives votes from a majority, it becomes the leader.

**Timeouts**: We use randomized election timeouts (150-300ms) to prevent split votes. If multiple nodes start elections simultaneously, the randomization ensures one will likely timeout first and win.

```go
func (s *Server) resetElectionTimerLocked() {
    // Randomized timeout prevents split votes
    timeout := time.Duration(150+rand.Intn(150)) * time.Millisecond
    t := time.NewTimer(timeout)
    s.electionTimer = t

    curTerm := s.currentTerm
    go func(timer *time.Timer, term int32) {
        <-timer.C
        s.onElectionTimeout(term)
    }(t, curTerm)
}

func (s *Server) onElectionTimeout(observedTerm int32) {
	s.mu.Lock()
	log.Printf("onElectionTimeout - currentTerm: %d, observedTerm: %d, nodeRole: %s", s.currentTerm, observedTerm, s.nodeRole)

	// If term changed due to RPCs and timer is stale, ignore
	if s.currentTerm != observedTerm {
		s.mu.Unlock()
		return
	}
	if s.nodeRole == roleLeader {
		s.mu.Unlock()
		return
	}
	// Start election
	s.currentTerm++
	s.votedFor = int32(s.id)
	s.nodeRole = roleCandidate
	s.persistState()
	term := s.currentTerm
	peers := append([]int(nil), s.peers...)
	s.resetElectionTimerLocked()
	s.mu.Unlock()

	votes := 1 // vote for self
	majority := (len(peers)+1)/2 + 1

	var wg sync.WaitGroup
	var votesMu sync.Mutex
	won := make(chan struct{}, 1)

	for _, peer := range peers {
		wg.Add(1)
		go func(pid int) {
			defer wg.Done()
			ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
			defer cancel()
			reply, ok := s.sendRequestVote(ctx, pid, &pb.ReqVoteArg{Term: term, CandidateId: int32(s.id)})
			if !ok {
				return
			}
			s.mu.Lock()
			if reply.Term > s.currentTerm {
				s.becomeFollowerLocked(reply.Term)
				s.mu.Unlock()
				return
			}
			// Only count if still candidate in same term
			if s.nodeRole == roleCandidate && s.currentTerm == term && reply.VoteYes {
				s.mu.Unlock()
				votesMu.Lock()
				votes++
				if votes >= majority {
					select {
					case won <- struct{}{}:
					default:
					}
				}
				votesMu.Unlock()
				return
			}
			s.mu.Unlock()
		}(peer)
	}

	go func() {
		wg.Wait()
		// If not won, nothing to do; another timeout will trigger
	}()

	select {
	case <-won:
		s.mu.Lock()
		if s.nodeRole == roleCandidate && s.currentTerm == term {
			s.becomeLeaderLocked()
		}
		s.mu.Unlock()
	case <-time.After(3 * time.Second):
		// let next timeout handle retry
	}
}
```

**Other Clock Types**:

- **Lamport Clocks**: Logical clocks that order events without physical time. Used in distributed systems for causal ordering.
- **Vector Clocks**: Extend Lamport clocks to detect causal relationships between events.
- **TrueTime**: Google's approach in [Spanner](https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf) using GPS and atomic clocks to bound clock uncertainty

**Why Raft Doesn't Use System Time**: One of the hardest lessons in distributed systems is that physical clocks are liars. Drifts in Network Time Protocol (NTP) can cause one server to think it's 10:00 AM while another thinks it's 9:59 AM. If we relied on timestamps for ordering, we'd violate consistency.

Instead, Raft uses Term as Logical Clocks. The `CurrentTerm` acts as a monotonically increasing counter that orders "eras" of leadership. This allows Raft to separate Safety (which never relies on physical time) from Liveness (which relies on timeouts).

>**Spanner breaks this rule by using TrueTime (GPS + Atomic Clocks) to bound clock uncertainty and guarantee a global ordering (external consistency).

**Failure Detectors**: Raft uses timeouts as a failure detector—if we don't hear from the leader, we assume it's failed. Other systems use more sophisticated detectors:

- **[Falcon](https://www.cs.utexas.edu/falcon/papers/falcon-sosp11.pdf)**: Uses network latency to detect failures faster
- **RAPID**: Adapts timeout values based on network conditions

### Safety and Raft Guarantees

Raft provides five critical safety guarantees:

1. **Election Safety**: At most one leader can be elected in a given term. This prevents split-brain.

2. **Leader Append-Only**: Leaders never overwrite or delete entries in their logs. They only append.

3. **Log Matching**: If two logs contain an entry with the same index and term, they are identical up to that point.

4. **Leader Completeness**: If a log entry is committed in a given term, it will be present in all future leaders. This ensures committed data is never lost.

5. **State Machine Safety**: All servers apply the same sequence of log entries to their state 

**Crash Safety via Persistence**: We persist critical state (current term, voted-for, log) to disk using atomic writes:

```go
func (s *Server) persistState() {
    // Atomic write pattern: temp file → fsync → rename
    tempFile := s.persistentStatePath + ".tmp"
    f, err := os.Create(tempFile)
    if err != nil {
        return
    }

    // Write state
    data, _ := json.Marshal(state)
    f.Write(data)

    // Force to disk (critical for crash safety)
    f.Sync()
    f.Close()

    // Atomic rename (filesystem guarantees)
    os.Rename(tempFile, s.persistentStatePath)
}
```

This three-step process (write to temp, fsync, rename) ensures state files are never corrupted, even if the process crashes mid-write.

Raft prevents split-brain by requiring a majority. Only the majority partition can elect a leader and commit entries. The minority partition cannot make progress, preventing conflicting operations.

When a partition heals, nodes in the minority partition will have stale logs. The new leader (from the majority) will replicate its log to these nodes, overwriting any conflicting entries. This is safe because the leader's log is guaranteed to contain all committed entries.

### Performance Considerations

**Heartbeat Intervals**: We use 100ms heartbeat intervals. Shorter intervals detect failures faster but increase network traffic. Longer intervals reduce traffic but increase failover time.

**Election Timeouts**: 150-300ms randomized timeouts balance between fast failover and stability. Too short, and we get spurious elections. Too long, and failover takes too long.

**Latency vs. Stability Trade-off**: There's a fundamental trade-off between latency and stability. We can reduce latency by committing entries faster, but this requires more frequent communication and increases the chance of conflicts.

In our implementation, we achieve:

- **Leader Election**: < 1 second in typical scenarios
- **Replication Latency**: ~100ms heartbeat interval
- **Read Latency**: Sub-millisecond (local state machine access)
- **Recovery Time**: Seconds for full cluster restart

## Future Work

While our current implementation provides a solid foundation, there are several enhancements that would make it production-ready at scale:

### Sharding for Scale

Currently, our system is a single shard which limits scalability. To scale to petabytes like Google BigTable, we would need to shard the data. Instead of `mod n` hashing (which breaks when a server is added), we would implement [Consistent Hashing](https://en.wikipedia.org/wiki/Consistent_hashing#:~:text=In%20computer%20science%2C%20consistent%20hashing,the%20number%20of%20keys%20and) (like [Chord](https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf) or Dynamo). This maps both keys and nodes to a circular keyspace (the "Ring"). When a node is added, it only steals keys from its immediate neighbor, minimizing data movement. Each Raft group would effectively act as a Tablet Server in BigTable architecture.

<div class="image-container"><img src="/assets/images/raft/consistent-hashing.png" style="width:1200px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Fig-6: Consistent hashing is used to dynamically balance load and reduce data movement during node addition/removal. </em></p>

### Log Compaction

As the log grows, storing every operation becomes expensive. **Log compaction** periodically creates snapshots of the state machine and removes old log entries. This is critical for long-running systems.

### Snapshotting

**Snapshots** are point-in-time copies of the state machine. When a new node joins or a node recovers from a crash, it can load a snapshot and then replay only the recent log entries, rather than replaying the entire history.

### Membership Changes

Currently, cluster membership is static. **Dynamic membership** would allow adding or removing nodes without downtime. This requires careful handling to prevent split-brain during configuration changes. Raft's approach uses a two-phase process to ensure safety.

### Read-Only Replicas

While all nodes can serve reads in our current implementation, we could optimize by having **read-only replicas** that don't participate in consensus but can serve read requests. This would improve read scalability without affecting write performance.

## Conclusion

Building Raft from scratch taught us that distributed systems are fundamentally about managing uncertainty. We can't assume messages will be delivered, nodes will stay alive, or the network will remain connected. Instead, we must design systems that are correct even when everything goes wrong.

Raft's genius is in its simplicity. By separating concerns (leader election, log replication, safety) and making the algorithm understandable, it enables practical implementations that provide strong consistency guarantees. Our implementation demonstrates that with careful attention to concurrency, persistence, and error handling, it's possible to build a production-quality distributed system.

Concepts such as consensus, the CAP theorem, failure models, and consistency guarantees are the foundation of modern distributed systems. Whether you're using etcd for Kubernetes coordination, Consul for service discovery, or building your own distributed database, these principles apply.

As distributed systems continue to evolve, new challenges emerge: handling Byzantine failures at scale, providing consistency across global deployments, and balancing performance with correctness. But the fundamental problem remains: how do we build systems that are correct, available, and performant in an uncertain world?

The answer, as Raft shows us, is through careful design, clear guarantees, and relentless attention to the edge cases that make distributed systems so challenging and so fascinating.

---

**Disclaimer**: To preserve academic integrity, we are maintaining our Raft codebase private. Please contact me on [LinkedIn](https://www.linkedin.com/in/william-g-sullivan/) if you would like access to the full codebase.
