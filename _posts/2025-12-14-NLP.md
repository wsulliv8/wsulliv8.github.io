---
layout: post
title: Low-Rank Adaptation Improves Adversarial Robustness in Fine-Tuned Language Models
image: /assets/images/nlp/paper.png
excerpt: How my research partner and I implemented three disparate dataset artifact mitigation techniques and issues we ran into along the way.
---

# Low-Rank Adaptation Improves Adversarial Robustness in Fine-Tuned Language

This blog should act as a compliment to my [research paper](https://wsulliv8.github.io/assets/docs/NLP_Paper.pdf) of the same name. While the paper explains the theory, analysis, and results in-depth, it does not touch on implementation details. Considering the implementation and training was where I spent the most time pulling my hair out, I expect this blog may be of use to someone implementing similar strategies or conducting research in this domain.

For the full codebase, feel free to check out the [GitHub repo](https://github.com/wsulliv8/dataset-artifacts).

---

## Part 1: Building the Baseline Pipeline

### Full Fine-Tuning

Our first step was getting a solid baseline working. We started with [HuggingFace Transformers](https://huggingface.co/docs/transformers/en/index), which made loading ELECTRA-small straightforward:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(
    "google/electra-small-discriminator",
    num_labels=3  # For NLI: entailment, neutral, contradiction
)
tokenizer = AutoTokenizer.from_pretrained(
    "google/electra-small-discriminator",
    use_fast=True
)
```

Note that you must use `AutoModelForQuestionAnswering` if training a QA model.

The ELECTRA paper reported 79.7% accuracy on MNLI after 3 epochs. Our initial attempt with their exact hyperparameters (batch size 32, learning rate 3e-4) only reached 76% after 5 epochs. That 5% gap was concerning.

After digging into the code and training dynamics, we discovered the issue: batch size. By doubling the batch size from 32 to 64, we stabilized gradient estimates and eliminated the early training instability we were seeing. Suddenly, we hit 78.5% after just 1.5 epochs.

```python
# Our final baseline configuration
args = {
    "batch_size": 64,      # Doubled from paper's 32
    "epochs": 5,
    "lr": 3e-4,
    "max_length": 128,
    "warmup_ratio": 0.1
}
```

Always validate your baseline first. If you can't reproduce published results, something may be wrong with your setup.

### Structuring the Code

Early on, we made a crucial decision: keep the code modular and extensible. We organized everything around a main `run.py` script that handles:

1. Model loading (full fine-tuning, LoRA, or ensemble)
2. Dataset preprocessing
3. Trainer configuration
4. Evaluation pipeline

This structure paid dividends when we added LoRA and ensemble methods later. The key was separating concerns:

```python
def load_model(args):
    """Load base model, optionally wrap with LoRA"""
    model = AutoModelForSequenceClassification.from_pretrained(
        args.model, num_labels=3
    )

    if args.lora_rank:
        # LoRA configuration (we'll cover this next)
        ...

    return model

def preprocess(args, tokenizer, dataset):
    """Handle dataset-specific preprocessing"""
    if args.task == "nli":
        return prepare_dataset_nli(examples, tokenizer, args.max_length)
    elif args.task == "qa":
        return prepare_train_dataset_qa(examples, tokenizer)
```

Design for extension, not just the current experiment. We didn't know we'd add ensemble methods when we started, but the modular structure made it easy.

Our `prepare_dataset_nli` function handles tokenizing the unaltered examples in addition to `hypothesis_only` examples. For `hypothesis_only` examples, do not simply set the premise to a blank string - the tokenizer needs to tokenize _only_ the hypothesis (we made this mistake).

```python
# This function preprocesses an NLI dataset, tokenizing premises and hypotheses.
def prepare_dataset_nli(examples, tokenizer, max_seq_length=None):
    max_seq_length = (
        tokenizer.model_max_length if max_seq_length is None else max_seq_length
    )

    premise = examples["premise"] if "premise" in examples else examples["sentence1"]
    hypothesis = (
        examples["hypothesis"] if "hypothesis" in examples else examples["sentence2"]
    )

    tokenized_examples = tokenizer(
        premise,
        hypothesis,
        truncation=True,
        max_length=max_seq_length,
        padding="max_length",
    )

    hypothesis_only = tokenizer(
        hypothesis,
        truncation=True,
        max_length=max_seq_length,
        padding="max_length",
    )

    tokenized_examples["hypothesis_only_input_ids"] = hypothesis_only["input_ids"]
    tokenized_examples["hypothesis_only_attention_mask"] = hypothesis_only[
        "attention_mask"
    ]
    if "token_type_ids" in hypothesis_only:
        tokenized_examples["hypothesis_only_token_type_ids"] = hypothesis_only[
            "token_type_ids"
        ]

    if "gold_label" in examples:
        label_map = {
            "entailment": 0,
            "non-entailment": 1,
        }
        tokenized_examples["label"] = [
            label_map.get(str(label).lower(), 1) for label in examples["gold_label"]
        ]
    else:
        tokenized_examples["label"] = examples["label"]

    return tokenized_examples

```

---

## Part 2: Implementing LoRA

### Parameter-Efficient Fine-Tuning with LoRA

The HuggingFace PEFT library made it straightforward to implement LoRA, but there were still important decisions to make.

LoRA adds low-rank adapters to specific layers. The question is: which ones? We targeted the attention and feed-forward layers:

```python
from peft import LoraConfig, TaskType, get_peft_model

config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=args.lora_rank,              # Rank of low-rank matrices
    lora_alpha=args.lora_alpha,    # Scaling parameter (default: 32)
    target_modules=["query", "key", "value", "dense"],  # Attention & FFN
    exclude_modules="classifier"   # Exclude task head
)

model = get_peft_model(model, config)
model.print_trainable_parameters()  # See parameter reduction
```

We excluded embedding layers since we found that including embeddings didn't help and increased parameters unnecessarily.

We also excluded the classifier head as the classifier needs full fine-tuning to learn task-specific patterns. LoRA on the encoder is sufficient.

**Learning rate scaling**

We read [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/) by John Schulman, which recommends scaling the learning rate by 10× for LoRA. With fewer trainable parameters, each parameter update has less impact. Scaling the learning rate compensates.

```python
# Full fine-tuning
lr = 3e-4

# LoRA fine-tuning
lr = 10 * 3e-4  # = 3e-3
```

This single change made LoRA competitive with full fine-tuning on in-domain accuracy, and we discovered it actually _improved_ robustness to adversarial triggers.

LoRA's parameter constraints act as implicit regularization. By limiting the model's ability to make arbitrary weight updates, we prevent overfitting to spurious lexical patterns in the training data. The model is forced to rely on its robust pre-trained understanding rather than memorizing dataset artifacts.

---

## Part 3: Ensemble Debiasing Implementation

### Building a Custom Trainer

Ensemble-based debiasing required a custom trainer class (implemented in the `ensemble-res` branch), since we needed to:

1. Run both the main model and a hypothesis-only "expert" model
2. Combine their logits during training
3. Handle gradients correctly for both models

We extended HuggingFace's `Trainer` class to create `ResidualLossTrainer`. Here's the core concept:

```python
class ResidualLossTrainer(Trainer):
    """
    Trainer that implements ensemble-based debiasing by combining main and hypothesis-only logits.

    This implements the DRiFt (Dataset Residual Fitting) approach where:
    1. A hypothesis-only model captures dataset artifacts (word-label correlations)
    2. The main model learns the "residual" - patterns the hypothesis-only model can't explain
    3. Logits are combined: combined_logits = main_logits + λ * hypothesis_logits

    If the hypothesis-only model can predict well, that's likely due to
    artifacts. The main model must find additional signal in the premise to justify predictions.
    """

    HYPOTHESIS_PREFIX = "hypothesis_only_"

    def __init__(
        self,
        *args,
        hypothesis_only_model,
        bias_weight=1.0,
        normalize_bias_logits=False,
        scale_bias_logits=False,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

        # Store the hypothesis-only expert model (trained separately on hypothesis-only features)
        # This model captures dataset artifacts like word-label correlations
        self.hypothesis_only_model = hypothesis_only_model

        # Move expert model to same device as main model (CPU/GPU)
        self.hypothesis_only_model.to(self.model.device)

        # Set expert model to evaluation mode (no batch norm updates, dropout disabled)
        self.hypothesis_only_model.eval()

        # Freeze all parameters in the expert model
        # We don't want to update it during training - it's our "bias detector"
        # Only the main model gets updated
        for param in self.hypothesis_only_model.parameters():
            param.requires_grad = False

        self.model_accepts_loss_kwargs = False

        # λ (lambda/bias_weight) controls how much we weight the hypothesis-only predictions
        # Higher λ = more aggressive debiasing, but may hurt in-domain accuracy
        # λ=1.0 is the standard DRiFt formulation
        self.bias_weight = bias_weight

        # Optional: Normalize logits before combining (L2 normalization)
        # Can help if logit scales differ significantly between models
        self.normalize_bias_logits = normalize_bias_logits

        # Optional: Dynamically scale bias logits to match main logits std
        # Alternative to normalization that preserves relative magnitudes
        self.scale_bias_logits = scale_bias_logits

        self.loss_fct = nn.CrossEntropyLoss()

    def _split_main_and_hypothesis_inputs(
        self, inputs: dict[str, Union[torch.Tensor, Any]]
    ):
        """
        Split inputs into main model inputs and hypothesis-only inputs.

        The dataset preprocessing creates hypothesis-only inputs with prefix "hypothesis_only_"
        (e.g., "hypothesis_only_input_ids", "hypothesis_only_attention_mask").
        This function separates them so each model gets the right inputs.
        """
        main_inputs = {}
        hypothesis_inputs = {}
        for key, value in inputs.items():
            if key.startswith(self.HYPOTHESIS_PREFIX):
                # Remove prefix to get the actual input key for the hypothesis-only model
                hypothesis_key = key[len(self.HYPOTHESIS_PREFIX) :]
                hypothesis_inputs[hypothesis_key] = value
            elif key != "labels":
                # Main model gets all inputs except labels and hypothesis-only prefixed keys
                main_inputs[key] = value
        return main_inputs, hypothesis_inputs

    def _forward_with_bias(
        self,
        model: nn.Module,
        inputs: dict[str, Union[torch.Tensor, Any]],
        labels: Optional[torch.Tensor] = None,
    ):
        """
        Runs both the main and hypothesis-only models and combines their logits.

        Steps:
        1. Split inputs into main (premise+hypothesis) and hypothesis-only
        2. Forward pass through main model (normal training, gradients enabled)
        3. Forward pass through hypothesis-only model (no gradients, frozen)
        4. Optionally normalize/scale logits if scales differ
        5. Combine logits: main + λ * hypothesis_only

        Returns: (outputs, combined_logits, hypothesis_only_logits, main_logits)
        """
        # Step 1: Split inputs - main model gets premise+hypothesis, expert gets only hypothesis
        main_inputs, hypothesis_inputs = self._split_main_and_hypothesis_inputs(inputs)

        # Step 2: Forward pass through main model (sees both premise and hypothesis)
        # This model can learn true reasoning patterns
        outputs = model(**main_inputs)
        hypothesis_forward_inputs = hypothesis_inputs

        # Step 3: Forward pass through hypothesis-only expert (no gradients)
        # This model only sees hypothesis, so it captures dataset artifacts
        # torch.no_grad() prevents gradient computation (saves memory and computation)
        with torch.no_grad():
            hypothesis_only_logits = self.hypothesis_only_model(
                **hypothesis_forward_inputs
            ).logits

        # Step 4: Optional logit normalization/scaling
        # Sometimes the two models produce logits at different scales
        if self.normalize_bias_logits:
            # L2 normalize both logit vectors before combining
            def _normalize(logits):
                norm = torch.linalg.vector_norm(logits, dim=-1, keepdim=True)
                return logits / (norm + 1e-6)

            main_logits = _normalize(outputs.logits)
            hypothesis_only_logits = _normalize(hypothesis_only_logits)
        elif self.scale_bias_logits:
            # Dynamic scaling: match bias logits std to main logits std
            # Preserves relative magnitudes while matching scales
            main_logits = outputs.logits
            main_std = main_logits.std()
            bias_std = hypothesis_only_logits.std()
            if bias_std > 1e-6:
                hypothesis_only_logits = hypothesis_only_logits * (main_std / bias_std)
        else:
            # No normalization - use raw logits
            main_logits = outputs.logits

        # Step 5: Combine logits using DRiFt formulation
        # Main model learns: "What can I predict that the hypothesis-only model can't?"
        # Higher bias_weight (λ) means we penalize the main model more for agreeing
        # with artifact-based predictions
        combined_logits = main_logits + self.bias_weight * hypothesis_only_logits
        outputs.logits = combined_logits

        return outputs, combined_logits, hypothesis_only_logits, main_logits

    def compute_loss(
        self,
        model: nn.Module,
        inputs: dict[str, Union[torch.Tensor, Any]],
        return_outputs: bool = False,
        num_items_in_batch: Optional[torch.Tensor] = None,
    ):
        """
        Compute loss using combined logits from main model and hypothesis-only expert.

        If the hypothesis-only model can predict the label well, that's
        likely due to dataset artifacts (word-label correlations). We want the main
        model to learn the "residual" - what the hypothesis-only model can't explain.

        The loss is computed on the combined logits, which forces the main model to
        either:
        - Find additional signal in the premise to justify artifact-based predictions
        - Learn to disagree when artifact-based predictions are wrong
        """
        labels = inputs["labels"]
        # Remove labels from model inputs (they're used separately for loss computation)
        model_inputs = {k: v for k, v in inputs.items() if k != "labels"}

        # Forward pass through both models and combine logits
        outputs, combined_logits, _, _ = self._forward_with_bias(
            model, model_inputs, labels=labels
        )

        # Compute cross-entropy loss on the combined logits
        # The model is trained to minimize this loss, which encourages it to learn
        # patterns beyond what the hypothesis-only model can capture
        loss = self.loss_fct(
            combined_logits.view(-1, self.model.config.num_labels),  # Flatten for loss
            labels.view(-1)  # Flatten labels to match
        )

        # Optional: Normalize loss by batch size (for variable-length batches)
        if num_items_in_batch is not None:
            loss = loss / num_items_in_batch

        return (loss, outputs) if return_outputs else loss

```

The key concept employed is residual debiasing by combining logits before computing loss: `combined_logits = main_logits + λ * hypothesis_logits`. This lets the main model learn the "residual" that the hypothesis-only model can't explain.

**Handling hypothesis-only inputs**

The dataset preprocessing needed to create separate inputs for the hypothesis-only model. We used a special prefix for these keys in `prepare_dataset_nli`:

```python
    hypothesis_only = tokenizer(
        hypothesis,
        truncation=True,
        max_length=max_seq_length,
        padding="max_length",
    )

    tokenized_examples["hypothesis_only_input_ids"] = hypothesis_only["input_ids"]
    tokenized_examples["hypothesis_only_attention_mask"] = hypothesis_only[
        "attention_mask"
    ]
    if "token_type_ids" in hypothesis_only:
        tokenized_examples["hypothesis_only_token_type_ids"] = hypothesis_only[
            "token_type_ids"
        ]
```

**Memory management**

Running two models simultaneously doubled memory usage. We had to:

- Keep the hypothesis-only model in eval mode (no gradients)
- Use `torch.no_grad()` for its forward pass
- Ensure both models were on the same device

**The trade-off**

Ensemble debiasing gave us explicit control over the bias-robustness trade-off via the `bias_weight` (λ) parameter. Higher λ = more debiasing, but we found it often reduced in-domain accuracy. This was a clear trade-off.

In contrast, LoRA improved both robustness _and_ accuracy without any explicit trade-off parameter.

---

## Part 4: HANS Integration and Robustness Evaluation

### Integrating the HANS Dataset

[HANS (Heuristic Analysis for NLI Systems)](https://arxiv.org/pdf/1902.01007) is a challenge set designed to expose spurious heuristic biases. It has a specific format with metadata about heuristics, subcases, and templates.

Our evaluation pipeline needed to:

1. Load HANS in the correct format
2. Run predictions
3. Analyze performance by heuristic category

> Note: The HANS dataset has two labels vice three, so we map the labels in `prepare_dataset_nli`.

```python
def prepare_dataset_nli(examples, tokenizer, max_seq_length=None):
    """Handle both standard NLI and HANS format"""

    # HANS uses "gold_label", standard NLI uses "label"
    if "gold_label" in examples:
        label_map = {"entailment": 0, "non-entailment": 1}
        tokenized["label"] = [
            label_map.get(str(l).lower(), 1) for l in examples["gold_label"]
        ]
    else:
        tokenized["label"] = examples["label"]

    return tokenized
```

We then concatenated the HANS training dataset to the MNLI training dataset in `preprocess`:

```python

    if hans_train_dataset:
        # Rename columns to match the NLI dataset format
        hans_train_dataset = hans_train_dataset.rename_column("sentence1", "premise")
        hans_train_dataset = hans_train_dataset.rename_column("sentence2", "hypothesis")
        hans_train_dataset = hans_train_dataset.rename_column("gold_label", "label")
        hans_train_dataset = hans_train_dataset.rename_column("pairID", "idx")

        # Remove columns that are not needed
        hans_train_dataset = hans_train_dataset.remove_columns(
            [
                "sentence1_binary_parse",
                "sentence2_binary_parse",
                "sentence1_parse",
                "sentence2_parse",
                "heuristic",
                "subcase",
                "template",
                "idx",
            ]
        )
        train_dataset = train_dataset.remove_columns(["idx"])

        def map_label(example):
            if example["label"] == "entailment":
                example["label"] = 0
            elif example["label"] == "non-entailment":
                example["label"] = 1
            return example

        # Map 2 labels to 3 labels
        hans_train_dataset = hans_train_dataset.map(
            map_label, num_proc=NUM_PREPROCESSING_WORKERS
        )

        train_dataset = datasets.concatenate_datasets(
            [train_dataset, hans_train_dataset]
        )
        train_dataset = train_dataset.shuffle(seed=42)

```

### Per-Heuristic Analysis

We used the analysis script provided by [McCoy et. al.](https://github.com/tommccoy1/hans) to analyze our model's performance against heuristics.

```python
# From evaluate_heur_output.py
def analyze_by_heuristic(predictions, correct_labels, heuristic_labels):
    """Group predictions by heuristic category"""
    heuristic_results = defaultdict(lambda: {"correct": 0, "total": 0})

    for pred, correct, heuristic in zip(predictions, correct_labels, heuristic_labels):
        heuristic_results[heuristic]["total"] += 1
        if pred == correct:
            heuristic_results[heuristic]["correct"] += 1

    for heuristic, counts in heuristic_results.items():
        accuracy = counts["correct"] / counts["total"]
        print(f"{heuristic}: {accuracy:.4f}")
```

This revealed that:

- Baseline models were heavily biased toward lexical overlap
- The model trained on the inoculated dataset performed extremely well on the HANS validation dataset
- LoRA improved subsequence and constituent patterns but not lexical overlap
- Ensemble debiasing helped across the board but with accuracy trade-offs

---

## Part 5: Managing the Experimentation Workflow

### Analysis

To find [Universal Adversarial Triggers (UAT)](https://arxiv.org/pdf/1908.07125) in the MNLI dataset suitable for black box attack, we calculated word-label correlation in the MNLI training dataset:

```python
"""
1a. Calculate word-label correlations from MNLI training data.

This analysis identifies dataset artifacts - words in hypotheses that are strongly
correlated with specific labels. These correlations can be exploited by models
without understanding the actual semantic relationship between premise and hypothesis.

For example, words like "never" or "nobody" might appear more often in contradiction
examples, allowing a model to predict contradiction based on word presence alone.
"""
import re
from collections import defaultdict
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from datasets import load_dataset

# Load MNLI training data
mnli_train = load_dataset("glue", "mnli", split="train")

# Initialize counters for each label
label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
label_word_counts = {
    label: defaultdict(int) for label in label_names
}

def tokenize(text: str) -> set[str]:
    """Extract unique tokens from text, excluding stop words."""
    tokens = re.findall(r"[A-Za-z']+", text.lower())
    return {token for token in tokens if token and token not in ENGLISH_STOP_WORDS}

# Count word occurrences per label in hypotheses
# This reveals which words are associated with which labels
for example in mnli_train:
    label = example["label"]
    unique_tokens = tokenize(example["hypothesis"])
    counts_for_label = label_word_counts[label]
    for token in unique_tokens:
        counts_for_label[token] += 1

entailment_counts = label_word_counts[0]
neutral_counts = label_word_counts[1]
contradiction_counts = label_word_counts[2]

# Calculate total occurrences across all labels
total_counts = defaultdict(int)
for counts in label_word_counts.values():
    for token, count in counts.items():
        total_counts[token] += count

# Compute conditional probabilities: P(label | word)
# High probability indicates a strong artifact - the word predicts the label
prob_contradiction_given_word = {}
prob_entailment_given_word = {}
prob_neutral_given_word = {}
for token, total in total_counts.items():
    contradiction_count = contradiction_counts.get(token, 0)
    prob_contradiction_given_word[token] = contradiction_count / total if total else 0.0

    entailment_count = entailment_counts.get(token, 0)
    prob_entailment_given_word[token] = entailment_count / total if total else 0.0

    neutral_count = neutral_counts.get(token, 0)
    prob_neutral_given_word[token] = neutral_count / total if total else 0.0

```

Using the top potential trigger words, we prepended each word to the hypothesis of each MNLI training dataset example before running evaluation:

```python
"""
1b) Black box model vocab attack - Universal Adversarial Triggers (UATs)

This tests whether the trained model is vulnerable to trigger words.
For each candidate word, we:
1. Prepend it to all hypotheses in the validation set
2. Evaluate the model's accuracy on this "triggered" dataset
3. Low accuracy indicates the word successfully fools the model

This is a black-box attack because we don't need model internals - just predictions.
If accuracy drops significantly, it shows the model relies on word-label correlations
rather than true semantic understanding.
"""

for label, df in dfs.items():
  for _, row in df.iterrows():

    # Create triggered dataset by prepending word to hypothesis
    # This simulates an adversarial attack where the trigger word is inserted
    word = row["word"]

    def prefix_hypothesis(example):
      """Prepend trigger word to hypothesis to create adversarial example."""
      example['hypothesis'] = f"{word} {example['hypothesis']}"
      return example

    out_path = f"triggered_{label}_{word}.jsonl"
    triggered_dataset = mnli_dev.map(prefix_hypothesis)
    triggered_dataset.to_json(out_path)

    # Evaluate using trained model
    # Low accuracy = model is fooled by the trigger word (vulnerable to artifacts)
    result = subprocess.run([
      "python3", "run.py",
      "--do_eval",
      "--eval_only",
      "--task", "nli",
      "--dataset", f"triggered_{label}_{word}.jsonl",
      "--model", "baseline_location",
      "--output_dir", "./eval_temp/",
    ], capture_output=True, text=True)

    # Load results
	with open("./eval_temp/eval_metrics.json", "r") as f:
		metrics = json.load(f)
		accuracy = metrics.get("eval_accuracy", 0)
		results[word] = accuracy
		save_progress()
		print(f"Word: '{word}', Accuracy: {accuracy}")

```

---

## Part 6: Key Technical Learnings

### 1. Parameter Constraints as Regularization

The most surprising finding was that LoRA's parameter constraints improve robustness. Here's why:

Full fine-tuning can make arbitrary weight updates. During training, the model learns patterns that help on the training set, including spurious correlations (like "words that appear in both premise and hypothesis → entailment").

LoRA restricts updates to low-rank matrices. This constraint prevents the model from memorizing spurious patterns. Instead, it must rely on the robust features learned during pre-training.

### 2. Learning Rate Scaling for Parameter-Efficient Methods

LoRA has far fewer trainable parameters than full fine-tuning (~0.2M vs 14M). To compensate, we scale the learning rate:

```python
# Rule of thumb: scale LR proportionally to parameter reduction
full_finetuning_lr = 3e-4
parameter_ratio = trainable_params_peft / total_params_full
lora_lr = full_finetuning_lr / parameter_ratio  # Roughly 10x
```

This follows from the observation that with fewer parameters, each parameter needs to change more to have the same overall effect.

### 3. The Memory-Compute Trade-off

- **Full fine-tuning:** 14M parameters, straightforward training
- **LoRA:** 0.2M parameters, same compute, better robustness
- **Ensemble:** 28M parameters (2 models), 2× memory, explicit control but accuracy trade-offs

For single-GPU constraints, LoRA was the clear winner: best robustness, best accuracy, lowest memory footprint.

---

## Conclusion

Implementing these three methods taught us as much about ML engineering and research practices as it did about robustness. The code is available in our [GitHub repo](https://github.com/wsulliv8/dataset-artifacts)., and we hope these lessons help others avoid some of the pitfalls we hit.

---

**Authors:** Litan Li & Will Sullivan, UT Austin

For questions or feedback, open an issue on GitHub or reach out:

- **Litan Li**: [LinkedIn](https://www.linkedin.com/in/litanli/)
- **Will Sullivan**: [LinkedIn](https://www.linkedin.com/in/william-g-sullivan/)
