---
layout: post
title: Slab's first blog
---
# Application of the Singular Value Decomposition 


The Singular Value Decomposition (SVD) was first discovered in 1870s, but remains a foundational technique in numerical linear algebra, machine learning, and data science. Gene Golub, William Kahan, and Christian Reinsch developed efficient algorithms to compute the SVD in the 1960s and 1970s. Specfically, the [Golub-Reinsch](https://people.duke.edu/~hpgavin/SystemID/References/Golub+Reinsch-NM-1970.pdf) algorithm is still widely used to this day.

Common use-cases of the SVD include image compression, Principal Component Analysis (PCA), Linear Least Squares (LLS) approximation, hand-written digit classification, and recommendation systems. Before we dive into these applications, a bit of theory is in order.

## Theory

The SVD theorem states that *any* matrix $A\in \mathbb{C}^{m\times n}$ can be split into the following matrices:

- [Unitary matrix](#note-1) $U\in \mathbb{C}^{m\times m}$
- Unitary matrix $V\in \mathbb{C}^{n\times n}$
  and
- and a [[linear-algebra/Blog Research/SVD/2025-1-24-SVD#^fb7e2e|"nearly" diagonal]] matrix $\Sigma \in \mathbb{R}^{m\times n}$

[[linear-algebra/Blog Research/SVD/2025-1-24-SVD#^282b87|After computing]] the above matrices, the SVD is:
$$
A=U\Sigma V^{H}
$$
## Key Applications

With the theory out of the way, here are a few of the most common use-cases of SVD:
### Image Compression

The SVD is commonly used to determine the best "rank-$k$" approximation of an image for image compression.

To illustrate:
- Consider a gray-scale image as a matrix $A\in ^{m\times n}$ where each value represents the pixel shade at that location in the image. 
- choose $k$ columns from matrix $A$ and store in matrix $B$
- Create a matrix $X$ such that the columns of $X$ are the columns of matrix $A$ projected onto matrix $B$ using the pseudo-inverse of matrix $A$.
$$
x_{i}=(B^{T}B)^{-1}B^{T}a_{i}
$$
- Now, $A$ may be approximated with
			$$
A \approx BX
$$
- I turns out that the SVD provides the best rank-k approximation of an image.
	- Let $A=U\Sigma V^{H}$
	- Choose the first $k$ columns of matrices $U,\Sigma ,V^{H}$ and set to $B$
- A practical demonstration of image compression may be found later in this article.

### Linear Least Squares

Linear Least Squares (LLS) approximation is a method to find the "best-fitting" line to a set of data points. 

SVD may be used to efficiently compute the pseudoinverse of matrix $A$, denoted as $A^{\dagger}$.

The "best-fitting" line $\hat{x}$ such that $A\hat{x} \approx b$ is given by:$$
\hat{x}=A^{\dagger}b
$$
- Since computing the inverse of a matrix is computationally expensive, one can instead use the SVD:
$$
A^{\dagger}=V \Sigma^{\dagger}U^{H}
$$
### Principle Component Analysis

Principal Component Analysis (PCA) is a common technique used for dimensionality reduction. PCA identifies the directions (principal components) in a dataset that capture the most variance thereby reducing the quantity of data while retaining the maximal amount of information.

PCA is traditionally computed by computing a covariance matrix, $C$, of a [[Applications of the Singular Value Decomposition#^|centered]] matrix $X$ and then computing the eigen-decomposition of matrix $C$. *However*, there are a number of issues that can be solved by instead solving for the PCA via the SVD by doing the following:
- Compute the SVD of centered matrix $X$	$$
X=U\Sigma V^{H}$$
- The columns of $V$ correspond to the eigenvectors of $C$ (directions of maximum variance in the data)
- The singular values $\sigma_{0},\sigma_{1},\dots$ of $\Sigma$ are related to the eigenvalues of $C$ (variance explained by $i$-th principal component)

  and
  - The matrix $U\Sigma$ gives the coordinates of the new principal component space.
  - Finally, to reduce the dimensionality of $X$ to $k$ components, compute:  $$
X_{reduced}= XV_{k}
$$Why go through the extra steps? Well SVD is more numerically stable than eigen-decomposition for large datasets and SVD is much less computationally expensive than computing the covariance matrix directly. 

## Example: Image Compression
### Notes
 
{#note-1} (1) A square matrix $B\in \mathbb{C}^{m\times m}$ is said to be unitary if and only if $B^{H}B=I$
- $I\in R^{m\times m}$ is the identity matrix 
- $B^{H}$ is the Hermitian transpose of matrix $B$ (transpose then take complex conjugate...$B^{H}=(\bar{B})^{T}$).
- Unitary matrices have many crucial properties such as preserving length when applied in the 2-norm ($\lvert \lvert \dots \rvert \rvert_{2}$) and the ability to change basis from the set of standard basis vectors ($e_{0}, e_{1}$, etc.) to a set of new vectors.

(2) By "nearly" I mean ^fb7e2e
- $\Sigma=\begin{pmatrix}\Sigma_{TL} & \Sigma_{TR} \\ \Sigma _{BL} & \Sigma_{BR}\end{pmatrix}$ where $\Sigma_{TL}=\begin{pmatrix}\sigma_{0} & 0 & \dots & 0 \\ 0 & \sigma_{1} & \dots & 0 \\ \vdots  & \vdots  & \ddots  & \vdots  \\ 0 & 0 & \dots & \sigma_{r-1}\end{pmatrix}$ and the other quadrants of $\Sigma$ are 0 matrices of appropriate sizes. Note that $r$ in this context refers to the rank (or column space) of matrix $A$.
- The values of $\sigma$ in the above matrix are called the "singular values" of matrix $A$ and $\sigma_{0} \ge \dots \ge \sigma_{r-1} >0$.
- *Note*: in two dimensions, the singular values determine the semiaxes of the two dimensional ellipse obtained from applying some matrix to a vector ***picture***. This can be generalized for $n$-dimensional space.

(3) The SVD of $A$ typically costs $O(m^{3})$ operations (inefficient algorithms may take up to $O(m^{4})$ operations ).  ^282b87

(4) The mean of each column of matrix $X$ is $0$ (the mean of each "feature" is 0). 