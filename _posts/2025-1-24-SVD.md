---
layout: post
title: Applications of the Singular Value Decomposition
image: /assets/images/svd-equation.png
excerpt: From Principal Componenet Analysis to modern recommendation systems, the SVD is a power-house in numerical linear algebra and machine learning. Learn more about the theory and application of the SVD.
---

The Singular Value Decomposition (SVD) was first discovered in 1870s, but remains a foundational technique in numerical linear algebra, machine learning, and data science. Gene Golub, William Kahan, and Christian Reinsch developed efficient algorithms to compute the SVD in the 1960s and 1970s. Specfically, the <a href="https://people.duke.edu/~hpgavin/SystemID/References/Golub+Reinsch-NM-1970.pdf" target="_blank">Golub-Reinsch</a> algorithm is still widely used to this day.

Common use-cases of the SVD include image compression, Principal Component Analysis (PCA), Linear Least Squares (LLS) approximation, hand-written digit classification, and recommendation systems. Before we dive into these applications, a bit of theory is in order.

## Theory  
{: #theory} 
The SVD theorem states that *any* matrix $A\in \mathbb{C}^{m\times n}$ can be split into the following matrices:

- [Unitary matrix](#note-one) $U\in \mathbb{C}^{m\times m}$

- Unitary matrix $V\in \mathbb{C}^{n\times n}$
  
- A ["nearly" diagonal](#note-two) matrix $\Sigma \in \mathbb{R}^{m\times n}$

[After computing](#note-three) the above matrices, the SVD is:

$$
A=U\Sigma V^{H}
$$


<div class="image-container"><img src="/assets/images/SVD-circle.png" style="width:400px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Geometric interpretation of the SVD  </em><a href='#note-one'>1</a></p>
The above picture provides a two-dimensional depiction of how each matrix transforms red and yellow vectors (note the use of $M$ vice $A$). Matrix $U$ is an orthonormal basis for the range of $M$ while $V$ is an orthonormal basis for the domain of $M$. Notice that the major axis of the mapped oval is the first singular value, $\sigma_{1}$, and the minor is the second singular value, $\sigma_{2}$.
## Applications

### 1) Image Compression

The SVD is commonly used to determine the best "rank-$k$" approximation of an image for image compression.

To illustrate:

- Consider a gray-scale image as a matrix $A\in \mathbb{R}^{m\times n}$ where each value represents the pixel shade at that location in the image. 

- Choose any $k$ columns from matrix $A$ and store in matrix $B$.

- Create a matrix $X$ such that the columns of $X$ are the columns of matrix $A$ projected onto matrix $B$ using the pseudo-inverse of matrix $B$.

$$
x_{i}=(B^{H}B)^{-1}B^{H}a_{i}
$$

- Now, $A$ may be approximated with

$$A \approx BX$$
- The compression comes from storing the reduced matrices $B\in \mathbb{R}^{m\times k}$ and $X\in \mathbb{R}^{k\times n}$ vice $A\in \mathbb{R}^{m\times n}$. Choosing a lower value of $k$ (fewer columns to represent the image) will result in higher compression.

- It turns out that the SVD provides the best rank-$k$ approximation of an image.

	- Let $A=U\Sigma V^{H}$
	
	- Choose the *first* $k$ columns of matrices $U,\Sigma ,V^{H}$ and set to $B$
	
	- Since the singular values are in order of "importance" for the corresponding vector representing $A$, picking the first $k$ columns will be the best rank-$k$ approximation since the columns will capture the most variance for that dimension.

	- This fact is proved in the [Eckart-Young-Mirsky theorem](https://en.wikipedia.org/wiki/Low-rank_approximation)
	

{: #Applications-LLS}
### 2) Linear Least Squares

<div class="image-container"><img src="/assets/images/LLS.png" style="width:400px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>A simple example of finding a best-fitting linear model using LLS </em><a href='#note-two'>2</a></p>


Linear Least Squares (LLS) approximation is a method to find the "best-fitting" linear model to a set of data points by minimizing the sum of squared differences between the observed values and the values predicted by the model.

So, there is no $x$ that solves $Ax=b$, but one can find $\hat{x}$ that satisifies:

$$
\lvert \lvert b-A\hat{x} \rvert  \rvert _{2} = min_{x \in C^{n}}\lvert \lvert b-Ax \rvert  \rvert _{2}
$$

The SVD may be used to efficiently compute the pseudoinverse of matrix $A$, denoted as $A^{\dagger}$.

The "best-fitting" line $\hat{x}$ such that $A\hat{x} \approx b$ is given by

$$\hat{x}=A^{\dagger}b=(A^{H}A)^{-1}A^{H}b$$

- Since computing the inverse of a matrix is computationally expensive, one can instead use the [reduced](#note-four) SVD:

$$A^{\dagger}=V_{L} \Sigma_{TL}^{-1}U_{L}^{H}$$


### 3) Principle Component Analysis
{: #PCA}
<div class="image-container"><img src="/assets/images/PCA.png" style="width:400px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>PCA graph that clearly shows the first two principal components (directions) </em><a href='#note-three'>3</a></p>

Principal Component Analysis (PCA) is a common technique used for dimensionality reduction. PCA identifies the directions (principal components) in a dataset that capture the most variance thereby reducing the quantity of data while retaining the maximal amount of information.

PCA is traditionally computed by computing a covariance matrix, $C$, of a [centered](#note-four) matrix $X$ and then computing the [eigen-decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of matrix $C$. *However*, there are a number of issues that can be solved by instead solving for the PCA via the SVD by doing the following:

- Compute the SVD of centered matrix $X$	

$$X=U\Sigma V^{H}$$

- The columns of $V$ correspond to the eigenvectors of $C$ (directions of maximum variance in the data)

- The singular values $\sigma_{0},\sigma_{1},\dots$ of $\Sigma$ are related to the eigenvalues of $C$ (variance explained by $i$-th principal component)

  - The matrix $U\Sigma$ gives the coordinates of the new principal component space.

  - Finally, to reduce the dimensionality of $X$ to $k$ components, compute:  

  $$X_{reduced}= XV_{k}$$
  
Why go through the extra steps? 
- SVD is more [numerically stable](#note-six) than eigen-decomposition for large datasets

 - SVD is much less computationally expensive than computing the covariance matrix directly 

### 4) Handwritten Digit Classification

<div class="image-container"><img src="/assets/images/digit.jpg" style="width:400px;"></div>
<p style="font-size: 0.8rem; text-align: center;"><em>Example hand-written digits  </em><a href='#note-four'>4</a></p>


The SVD's dimensionality reduction capability in handwritten digit datasets such as [MNIST](https://en.wikipedia.org/wiki/MNIST_database) is paramount in classification. Each digit in MNIST is represented by a large number of pixel intensities, so SVD can reduce its dimensionality while retaining the most important information (the largest singular values). 

To do this the matrix of digit images, $A$, is decomposed into $A=U\Sigma V^{H}$ and the first $k$ singular values are chosen to be retained. The reduced dataset can then be input into a machine learning model such as a Support Vector Machine (SVM), Neural Network, or k-Nearest Neighbors (k-NN). 


### 5) Recommendation Systems

A typical movie recommendation systems may be composed of a [user-item interaction matrix](https://buomsoo-kim.github.io/recommender%20systems/2020/09/25/Recommender-systems-collab-filtering-12.md/#:~:text=User%2Ditem%20interaction%20matrix%20revisited,please%20refer%20to%20this%20posting.), $A$, where the rows are users, columns are items, and individual entries represent a rating (or other data point).   

Decomposing $A$ into its SVD identifies latent factors (the singular values) that can capture patterns such as genres or other specific preferences. Again, we will keep the first $k$ singular values to reduce dimensionality and improve the system's efficiency. 

SVD-based recommendation also handles sparse matrices well by choosing the most important latent factors and can uncover hidden patterns in user behavior.

[This blog](https://jaketae.github.io/study/svd/) dives further into SVD recommendation systems.
## Example: Image Compression

The following python script is an example of how the SVD may be used in image compression. Plug it into a Jupyter notebook and play around with it yourself.

{% highlight python %} 
import numpy as np
import matplotlib.pyplot as plt
from skimage import data
from skimage.color import rgb2gray

image = rgb2gray(data.cat())
print("Original image shape:", image.shape)

U, S, VT = np.linalg.svd(image, full_matrices=False)
{% endhighlight %}

Import the necessary packages and load an image of a cat (provided by scikit-image) then create the SVD of the image.

{% highlight python %} 
def reconstruct_image(k):
  S_k = np.diag(S[:k]) 
  return np.dot(U[:, :k], np.dot(S_k, VT[:k, :]))
{% endhighlight %}

Create a function that first creates a new diagonal matrix with $k$ singular values and then reforming the image via $SVD=U_{k}\Sigma_{k}V_{k}^{H}$

{% highlight python %} 
k_values = [5, 20, 50, 100]
fig, axes = plt.subplots(1, len(k_values) + 1, figsize=(15, 5))

axes[0].imshow(image, cmap="gray")
axes[0].set_title("Original")
axes[0].axis("off")

rows=300
columns = 451
print('Original image size: ' , rows*columns)
for i, k in enumerate(k_values):
  compressed_image = reconstruct_image(k)
  axes[i + 1].imshow(compressed_image, cmap="gray")
  axes[i + 1].set_title(f"k = {k}")
  axes[i + 1].axis("off")
  print("New image size:", k*(1+rows+columns))

plt.tight_layout()
plt.show()
{% endhighlight %}

Plot the original cat image and compressed image for various $k$ values. Here, $k$ will be the rank of resulting matrix (hence rank-$k$ approximation).

{% highlight python %} 
OUTPUT:
Original image shape: (300, 451) 
Original image size: 135300 
New image size: 3760 
New image size: 7520 
New image size: 15040 
New image size: 75200
{% endhighlight %}
<div class="image-container"><img src="/assets/images/cats.png" style="width:1200px;"></div>
### Closing Thoughts

The SVD is one of the most important tools in linear algebra and has widespread applications in data science and machine learning. If you'd like to deepen your theoretical understanding of the SVD, please reference the links listed below:
- [Linear Algebra: Foundation to Frontiers (MOOC)](http://ulaff.net/) - Robert van de Geijn, Margaret Myers
	- I am currently enrolled in the graduate version at UT Austin and can't recommend this material enough
- [MIT Undergraduate Linear Algebra ](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) - Gilbert Strang
- [Numerical Linear Algebra](https://www.stat.uchicago.edu/~lekheng/courses/309/books/Trefethen-Bau.pdf) - Lloyd Trefethen, David Bau

### Notes
 
 **(1)** A square matrix $B\in \mathbb{C}^{m\times m}$ is said to be unitary if and only if $B^{H}B=I$
 
- $I\in R^{m\times m}$ is the identity matrix 

- $B^{H}$ is the Hermitian transpose of matrix $B$ (transpose then take complex conjugate...$B^{H}=(\bar{B})^{T}$).

- Unitary matrices have many crucial properties such as preserving length when applied in the 2-norm ($\lvert \lvert \dots \rvert \rvert_{2}$) and the ability to preserve geometric properties if used for a change of basis (this is used in [householder QR factorization](https://kwokanthony.medium.com/detailed-explanation-with-example-on-qr-decomposition-by-householder-transformation-5e964d7f7656)) .  [Return](#theory)
 {: #note-one}  
 

**(2)** "Nearly" as in:
{: #note-two}

-  $$\Sigma=\begin{pmatrix}\Sigma_{TL} & \Sigma_{TR} \\ \Sigma _{BL} & \Sigma_{BR}\end{pmatrix}$$ where $$\Sigma_{TL}=\begin{pmatrix}\sigma_{0} & 0 & \dots & 0 \\ 0 & \sigma_{1} & \dots & 0 \\ \vdots  & \vdots  & \ddots  & \vdots  \\ 0 & 0 & \dots & \sigma_{r-1}\end{pmatrix}$$ and the other quadrants of $\Sigma$ are 0 matrices of appropriate sizes. Note that $r$ in this context refers to the rank (or column space) of matrix $A$. 

- The values of $\sigma$ in the above matrix are called the "singular values" of matrix $A$ and $\sigma_{0} \ge \dots \ge \sigma_{r-1} >0$.  [Return](#theory)

**(3)** Computing the SVD of $A$ typically costs $O(m^{3})$ operations (inefficient algorithms may take up to $O(m^{4})$ operations ). [Return](#theory)
{: #note-three}

**(4)** The [reduced SVD](https://www.cs.cornell.edu/courses/cs322/2008sp/stuff/TrefethenBau_Lec4_SVD.pdf)  partitions the SVD such that only the diagonal part of $\Sigma$ (the singular values) is used.

- The SVD of $A$ is:

$$A=\begin{pmatrix}
U_{L} &  U_{R}
\end{pmatrix}\begin{pmatrix}
\Sigma_{TL} & 0 \\
0 & 0
\end{pmatrix}\begin{pmatrix}
V_{L} & V_{R}
\end{pmatrix}^{H}$$

- The reduced SVD of $A$ is:

$$A=U_{L}\Sigma_{TL}V_{L}^{H}$$

- Here, the column space of $A$ is the column space of $U_{L}$ and the row space of $A$ is the column space of $V_{L}$. [Return](#Applications-LLS)
{: #note-four}   

**(4)** The mean of each column of matrix $X$ is $0$ (the mean of each "feature" is 0). [Return](#PCA)
{: #note-five}   

**(5)** Numerical stability is the sensitivity of a computation to errors introduced by floating point operations performed by computers. A computation is numerically stable if small changes in the input do not lead to large changes in the final result. These errors are often introduced via rounding errors, [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation), and overflow/underflow. [Return](#PCA)
{: #note-six}

## References 

**(1)** [Singular Value Decomposition - Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition) 
{: #ref-one}

**(2)** [Linear Least Squares - Wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares)
{: #ref-two}

**(3)** [Principal Component Analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)
{: #ref-three}

**(4)** [Handwritten Digit Classification Using Higher Order SVD](https://www.sciencedirect.com/science/article/abs/pii/S0031320306003542) 
{: #ref-four}

**(5)** [Linear Algebra: Foundation to Frontiers (MOOC)](http://ulaff.net/)
